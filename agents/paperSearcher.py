from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, START, END
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
import arxiv
import requests
import time
from pathlib import Path
import os
from langchain.chat_models import init_chat_model
import dotenv
from loguru import logger
from tools.timing import get_timing_logger, time_node
dotenv.load_dotenv(dotenv_path="/Users/chongyanghe/Desktop/DeepScientist/.env")

from utils.state import State

# ===================== 1. ç¬¬ä¸€ä¸ª Agent: Query Refinement Agent =====================
class QueryRefinementAgent:
    """Agent 1: æŸ¥è¯¢ä¼˜åŒ–ï¼ˆç¿»è¯‘+å¢å¼ºï¼‰"""
    
    def __init__(self, model_name: str = "deepseek-reasoner", temperature: float = 0.7):
        self.llm = init_chat_model(
                    model_name,
                    base_url=os.environ.get("OPENAI_BASE_URL", ""),
                    model_provider="openai",
                    extra_body={"chat_template_kwargs": {"enable_thinking": True}}
                )
        # è¯»å– prompt æ–‡ä»¶ï¼ˆå…¼å®¹è·¯å¾„ä¸å­˜åœ¨çš„æƒ…å†µï¼Œå†…ç½®é»˜è®¤ promptï¼‰
        prompt_path = Path(os.path.dirname(__file__)) / ".." / "prompt" / "queryTranslation.md"
        if prompt_path.exists():
            with open(prompt_path, "r", encoding="utf-8") as f:
                prompt_template = f.read()
        else:
            # é»˜è®¤ promptï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼‰
            prompt_template = """
            Please optimize the following query statement to be suitable for the ArXiv academic 
            search (if it is already in English, enhance the keywords): 
            Query statement: {query}
            """
        self.prompt_template = PromptTemplate.from_template(prompt_template)

    def refine_query(self, state: State) -> State:
        """
        æ¥æ”¶çŠ¶æ€å¯¹è±¡ï¼Œä¼˜åŒ–æŸ¥è¯¢åè¿”å›æ›´æ–°çš„çŠ¶æ€
        """
        # ä» state ä¸­æå–åˆå§‹æŸ¥è¯¢
        original_query = state["original_query"]
        # è°ƒç”¨ LLM ä¼˜åŒ–æŸ¥è¯¢
        prompt = self.prompt_template.format(query=original_query)
        response = self.llm.invoke(prompt)
        # æå–ä¼˜åŒ–åçš„æŸ¥è¯¢ï¼ˆå…³é”®ï¼šAIMessage éœ€è¦å– content å±æ€§ï¼‰
        refined_query = response.content.strip()
        # æ›´æ–° state å¹¶è¿”å›
        state["refined_query"] = refined_query

        return state

# ===================== 2. ç¬¬äºŒä¸ª Agent: Search Agent (ArXiv æœç´¢) =====================
class SearchAgent:
    """Agent 2: è®ºæ–‡æœç´¢ï¼ˆArXivï¼‰"""
    
    def __init__(self, model_name: str = "deepseek-chat", temperature: float = 0.7):
        self.llm = init_chat_model(
                    model_name,
                    base_url=os.environ.get("OPENAI_BASE_URL", ""),
                    model_provider="openai",
                    extra_body={"chat_template_kwargs": {"enable_thinking": True}}
                )

    def search_papers(self, state: State) -> State:
        """
        æ¥æ”¶çŠ¶æ€å¯¹è±¡ï¼Œæœç´¢è®ºæ–‡åè¿”å›æ›´æ–°çš„çŠ¶æ€
        """
        # ä» state ä¸­æå–ä¼˜åŒ–åçš„æŸ¥è¯¢
        refined_query = state["refined_query"]
        logger.info(f"ğŸ” ä½¿ç”¨ä¼˜åŒ–åæŸ¥è¯¢è¿›è¡Œ ArXiv æœç´¢ï¼š{refined_query}")
        # è°ƒç”¨ ArXiv API æœç´¢è®ºæ–‡
        paper_urls = self._search_arxiv(refined_query)
        # æ›´æ–° state å¹¶è¿”å›
        state["paper_urls"] = paper_urls
        return state

    def _search_arxiv(self, query: str) -> list:
        """ArXiv åº•å±‚æœç´¢é€»è¾‘"""
        urls = []
        try:
            search = arxiv.Search(
                query=query,
                max_results=1, # å¯ä¿®æ”¹
                sort_by=arxiv.SortCriterion.Relevance,
                sort_order=arxiv.SortOrder.Descending
            )
            for result in search.results():
                urls.append(result.pdf_url)
        except Exception as e:
            print(f"ArXiv æœç´¢å¤±è´¥ï¼š{e}")
        return urls

# ===================== 3. ç¬¬ä¸‰ä¸ª Agent: Downloader Agent (è®ºæ–‡ä¸‹è½½) =====================
class DownloaderAgent:
    """Agent 3: è®ºæ–‡ä¸‹è½½ï¼ˆPDFï¼‰"""
    
    def __init__(self, download_dir: str = os.path.join(os.path.dirname(__file__), "../outputs/papers"), max_papers: int = 10):
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.max_papers = max_papers
    
    def download_papers(self, state: State) -> State:
        """
        æ¥æ”¶çŠ¶æ€å¯¹è±¡ï¼Œä¸‹è½½è®ºæ–‡åè¿”å›æ›´æ–°çš„çŠ¶æ€
        """
        # ä» state ä¸­æå–è®ºæ–‡ URL åˆ—è¡¨
        paper_urls = state["paper_urls"]
        # ä¸‹è½½è®ºæ–‡
        downloaded_papers = self._download_papers(paper_urls)
        # æ›´æ–° state å¹¶è¿”å›
        state["downloaded_papers"] = downloaded_papers

        return state

    def _download_papers(self, paper_urls: list) -> list:
        """åº•å±‚ä¸‹è½½é€»è¾‘"""
        downloaded_papers = []
        for url in paper_urls[:self.max_papers]:
            try:
                # é¿å…è¯·æ±‚è¿‡å¿«è¢«å°
                time.sleep(1)
                response = requests.get(url, timeout=10)
                response.raise_for_status()  # æŠ›å‡º HTTP é”™è¯¯
                # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                filename = url.split("/")[-1] + ".pdf"
                filepath = self.download_dir / filename
                # å†™å…¥æ–‡ä»¶
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                # è®°å½•ä¸‹è½½ç»“æœ
                # downloaded_papers.append({
                #     "url": url,
                #     "file_path": str(filepath),
                #     "status": "success"
                # })
                downloaded_papers.append(filepath)
                print(f"æˆåŠŸä¸‹è½½ï¼š{filepath}")
            except Exception as e:
                print(f"ä¸‹è½½å¤±è´¥ {url}ï¼š{e}")
                # downloaded_papers.append({
                #     "url": url,
                #     "file_path": "",
                #     "status": f"failed: {str(e)}"
                # })

        return downloaded_papers

# ===================== 5. åˆ›å»º LangGraph å·¥ä½œæµ =====================
def build_literature_search_subgraph():
    """åˆ›å»ºå¹¶ç¼–è¯‘ langgraph å·¥ä½œæµ"""
    timing_logger, log_file = get_timing_logger(
        log_dir="./outputs/logs/paperSearcher",
        agent_name='paperSearcher'
    )
    timing_logger.info("="*60)
    timing_logger.info("Initialize paperSearcher subgraph")
    timing_logger.info("="*60)

    # åˆå§‹åŒ– Agent å®ä¾‹
    refine_agent = QueryRefinementAgent()
    search_agent = SearchAgent()
    download_agent = DownloaderAgent()

    refine_query_timed = time_node("paperSearcher", "refine_query", timing_logger)(refine_agent.refine_query)
    search_papers_timed = time_node("paperSearcher", "search_papers", timing_logger)(search_agent.search_papers)
    download_papers_timed = time_node("paperSearcher", "download_papers", timing_logger)(download_agent.download_papers)

    # åˆå§‹åŒ– StateGraphï¼ˆç»‘å®šçŠ¶æ€ç±»ï¼‰
    graph = StateGraph(State)

    # æ·»åŠ èŠ‚ç‚¹ï¼ˆæ ¸å¿ƒï¼šèŠ‚ç‚¹æ˜¯æ¥æ”¶ state å¹¶è¿”å› state çš„å‡½æ•°ï¼‰
    graph.add_node("refine_query", refine_query_timed)
    graph.add_node("search_papers", search_papers_timed)
    graph.add_node("download_papers", download_papers_timed)

    # è®¾ç½®å·¥ä½œæµè¾¹ï¼ˆå®šä¹‰æ‰§è¡Œé¡ºåºï¼‰
    graph.add_edge(START, "refine_query")          # å¼€å§‹ â†’ ä¼˜åŒ–æŸ¥è¯¢
    graph.add_edge("refine_query", "search_papers") # ä¼˜åŒ–æŸ¥è¯¢ â†’ æœç´¢è®ºæ–‡
    graph.add_edge("search_papers", "download_papers") # æœç´¢è®ºæ–‡ â†’ ä¸‹è½½è®ºæ–‡
    graph.add_edge("download_papers", END)         # ä¸‹è½½è®ºæ–‡ â†’ ç»“æŸ

    graph = graph.compile()

    # ç¼–è¯‘å·¥ä½œæµ
    return graph

# # ===================== 6. æ‰§è¡Œå·¥ä½œæµ =====================
# def build_literature_search_subgraph(original_query: str):
#     """
#     æ‰§è¡Œå·¥ä½œæµ
#     :param original_query: ç”¨æˆ·åˆå§‹æŸ¥è¯¢è¯­å¥
#     :return: æœ€ç»ˆçŠ¶æ€ï¼ˆåŒ…å«ä¸‹è½½ç»“æœï¼‰
#     """
#     # åˆ›å»ºå·¥ä½œæµ
#     sub_graph_1 = create_workflow()
#     # åˆå§‹åŒ– stateï¼ˆå°†åˆå§‹ query å­˜å…¥ stateï¼‰
#     initial_state = {"original_query": original_query}
#     # è¿è¡Œå·¥ä½œæµï¼ˆä¼ å…¥åˆå§‹ stateï¼‰
#     final_state = sub_graph_1.invoke(initial_state)
#     return final_state

# ===================== 7. æµ‹è¯•è¿è¡Œ =====================
if __name__ == "__main__":
    # ç¤ºä¾‹æŸ¥è¯¢
    user_query = "Quantum computing advancements"
    # æ‰§è¡Œå·¥ä½œæµ
    result = build_literature_search_subgraph(user_query)
    
    # æ‰“å°ç»“æœ
    print("="*50)
    print(f"åˆå§‹æŸ¥è¯¢ï¼š{result.original_query}")
    print(f"ä¼˜åŒ–åæŸ¥è¯¢ï¼š{result.refined_query}")
    print(f"æœç´¢åˆ°çš„è®ºæ–‡ URLï¼š{result.paper_urls}")
    print("ä¸‹è½½ç»“æœï¼š")
    for paper in result.downloaded_papers:
        print(f"- {paper['url']} â†’ {paper['status']} ({paper['file_path']})")
    print("="*50)

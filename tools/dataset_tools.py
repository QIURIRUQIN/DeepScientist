from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, PrivateAttr
from dataclasses import dataclass
from pathlib import Path
import os
import time
from loguru import logger

try:
    from langchain.tools import BaseTool
except ImportError:
    class BaseTool:
        name: str = ""
        description: str = ""
        args_schema: Any = None
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)
        def _run(self, **kwargs):
            raise NotImplementedError
        async def _arun(self, **kwargs):
            raise NotImplementedError

# ç¡®ä¿å¯¼å…¥çš„æ¨¡å—è¿”å›å€¼åŒ¹é…
from utils.dataset_download import KaggleAuthenticator, KaggleSearcher, KaggleDownloader, DatasetInfo
from utils.state import State

# æç®€é…ç½®ï¼ˆä»…ä¿ç•™å¿…è¦é¡¹ï¼‰
@dataclass
class KaggleToolConfig:
    config_path: Optional[str] = None
    default_download_path: str = "./datasets"
    auto_extract: bool = True

class KaggleDatasetInput(BaseModel):
    keyword: str = Field(description="æœç´¢å…³é”®è¯ï¼ˆå¿…å¡«ï¼‰")
    download_path: Optional[str] = Field(default=None, description="ä¸‹è½½è·¯å¾„")

class KaggleDatasetTool(BaseTool):
    name: str = "kaggle_dataset_downloader"
    description: str = """ä»…ä¸‹è½½1ä¸ªå«.csvçš„æ•°æ®é›†ï¼Œæ‰¾åˆ°åç«‹å³ç»ˆæ­¢æ‰€æœ‰æ“ä½œ"""
    args_schema: type[BaseModel] = KaggleDatasetInput
    _config: KaggleToolConfig = PrivateAttr()
    _authenticator: KaggleAuthenticator = PrivateAttr()
    _searcher: KaggleSearcher = PrivateAttr()
    _downloader: KaggleDownloader = PrivateAttr()

    def __init__(self, config: Optional[KaggleToolConfig] = None, **kwargs):
        super().__init__(** kwargs)
        self._config = config or KaggleToolConfig()
        try:
            self._authenticator = KaggleAuthenticator(self._config.config_path)
            self._searcher = KaggleSearcher(self._authenticator)
            self._downloader = KaggleDownloader(self._authenticator)
            # ä¿®å¤kaggle.jsonæƒé™
            kaggle_json = Path.home() / ".kaggle" / "kaggle.json"
            if kaggle_json.exists():
                os.chmod(kaggle_json, 0o600)
        except Exception as e:
            raise RuntimeError(f"Kaggleåˆå§‹åŒ–å¤±è´¥ï¼š{e}")

    @staticmethod
    def _check_csv_exists(path: str) -> bool:
        """æ£€æµ‹æ˜¯å¦æœ‰æœ‰æ•ˆ.csvæ–‡ä»¶"""
        if not path:
            return False
        p = Path(path)
        for csv_file in p.rglob("*.csv"):
            if csv_file.is_file() and csv_file.stat().st_size > 100:
                logger.info(f"âœ… æ‰¾åˆ°æœ‰æ•ˆCSVï¼š{csv_file}")
                return True
        return False

    @staticmethod
    def _optimize_keywords(keyword: str) -> List[str]:
        """
        ä¼˜åŒ–å…³é”®è¯ï¼Œç”Ÿæˆå¤šä¸ªæœç´¢ç­–ç•¥ï¼ˆä»ç²¾ç¡®åˆ°é€šç”¨ï¼‰
        ç­–ç•¥1: åŸå§‹å…³é”®è¯ï¼ˆæ¸…ç†åï¼‰
        ç­–ç•¥2: æå–æ ¸å¿ƒåè¯ï¼ˆå»é™¤ä¿®é¥°è¯ï¼‰
        ç­–ç•¥3: æ‹†åˆ†å¤åˆå…³é”®è¯ï¼Œåˆ†åˆ«å°è¯•
        ç­–ç•¥4: ä½¿ç”¨å•ä¸ªæ ¸å¿ƒè¯
        """
        import re
        
        # æ¸…ç†å…³é”®è¯ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦
        clean_keyword = re.sub(r'[^\w\s-]', ' ', keyword)
        clean_keyword = " ".join([w.strip() for w in clean_keyword.split() if w.strip()])
        
        keywords = [clean_keyword]  # ç­–ç•¥1: åŸå§‹å…³é”®è¯
        
        # ç­–ç•¥2: æå–æ ¸å¿ƒåè¯ï¼ˆå»é™¤å¸¸è§ä¿®é¥°è¯ï¼‰
        words = clean_keyword.split()
        # å¸¸è§ä¿®é¥°è¯åˆ—è¡¨ï¼ˆå¯ä»¥æ‰©å±•ï¼‰
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
                     'of', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be', 
                     'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 
                     'would', 'should', 'could', 'may', 'might', 'must', 'can', 'multi',
                     'classification', 'management', 'system', 'framework', 'model', 'method'}
        core_words = [w for w in words if w.lower() not in stop_words and len(w) > 2]
        if len(core_words) > 0:
            # ä¿ç•™å‰3-5ä¸ªæ ¸å¿ƒè¯
            core_keyword = " ".join(core_words[:5])
            if core_keyword != clean_keyword:
                keywords.append(core_keyword)
        
        # ç­–ç•¥3: å¦‚æœå…³é”®è¯åŒ…å«å¤šä¸ªéƒ¨åˆ†ï¼Œå°è¯•æ‹†åˆ†
        if len(words) > 2:
            # å°è¯•å‰åŠéƒ¨åˆ†
            first_half = " ".join(words[:len(words)//2 + 1])
            if first_half not in keywords:
                keywords.append(first_half)
            # å°è¯•ååŠéƒ¨åˆ†
            second_half = " ".join(words[len(words)//2:])
            if second_half not in keywords:
                keywords.append(second_half)
        
        # ç­–ç•¥4: ä½¿ç”¨å•ä¸ªæœ€é‡è¦çš„æ ¸å¿ƒè¯ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæˆ–æœ€é•¿çš„ï¼‰
        if core_words:
            # é€‰æ‹©æœ€é•¿çš„æ ¸å¿ƒè¯ï¼Œæˆ–è€…ç¬¬ä¸€ä¸ªæ ¸å¿ƒè¯
            single_keyword = max(core_words, key=len) if core_words else words[0] if words else ""
            if single_keyword and single_keyword not in keywords:
                keywords.append(single_keyword)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_keywords = []
        for kw in keywords:
            kw_lower = kw.lower()
            if kw_lower not in seen and kw.strip():
                seen.add(kw_lower)
                unique_keywords.append(kw)
        
        return unique_keywords

    def _run(self, **kwargs) -> tuple[str, str, str]:
        """
        ä¼˜åŒ–åçš„æ ¸å¿ƒé€»è¾‘ï¼šå¤šçº§é™çº§ç­–ç•¥ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°æ•°æ®é›†
        1. å°è¯•å¤šä¸ªå…³é”®è¯å˜ä½“ï¼ˆä»ç²¾ç¡®åˆ°é€šç”¨ï¼‰
        2. æ¯ä¸ªå…³é”®è¯å°è¯•å¤šä¸ªç»“æœï¼ˆmax_results=5ï¼‰
        3. é€‰æ‹©æœ€ä½³åŒ¹é…çš„æ•°æ®é›†
        4. éªŒè¯CSVæ–‡ä»¶å­˜åœ¨
        """
        # 1. è§£æå‚æ•°
        keyword = kwargs.get('keyword', '').strip()
        download_path = kwargs.get('download_path') or self._config.default_download_path
        os.makedirs(download_path, exist_ok=True)

        if not keyword:
            raise RuntimeError("å…³é”®è¯ä¸èƒ½ä¸ºç©ºï¼")

        # 2. ç”Ÿæˆå¤šä¸ªå…³é”®è¯ç­–ç•¥
        keyword_strategies = self._optimize_keywords(keyword)
        logger.info(f"ğŸ” åŸå§‹å…³é”®è¯ï¼š{keyword}")
        logger.info(f"ğŸ“‹ ç”Ÿæˆæœç´¢ç­–ç•¥ï¼š{keyword_strategies}")

        # 3. å¤šçº§é™çº§ç­–ç•¥ï¼šä¾æ¬¡å°è¯•æ¯ä¸ªå…³é”®è¯
        all_datasets = []
        tried_keywords = []
        
        for search_keyword in keyword_strategies:
            tried_keywords.append(search_keyword)
            logger.info(f"ğŸ” å°è¯•å…³é”®è¯ï¼š{search_keyword}")

            try:
                # æ¯ä¸ªå…³é”®è¯å°è¯•è·å–5ä¸ªç»“æœ
                datasets = self._searcher.search_by_keyword(search_keyword, max_results=5)
                if datasets:
                    logger.info(f"âœ… å…³é”®è¯'{search_keyword}'æ‰¾åˆ°{len(datasets)}ä¸ªç»“æœ")
                    all_datasets.extend(datasets)
                    # å¦‚æœæ‰¾åˆ°ç»“æœï¼Œç»§ç»­å°è¯•å…¶ä»–å…³é”®è¯ä»¥æ”¶é›†æ›´å¤šå€™é€‰
                else:
                    logger.warning(f"âš ï¸ å…³é”®è¯'{search_keyword}'æ— ç»“æœ")
            except Exception as e:
                logger.warning(f"âš ï¸ å…³é”®è¯'{search_keyword}'æœç´¢å‡ºé”™ï¼š{e}")
                continue
        
        # 4. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®é›†ï¼Œä½¿ç”¨é€šç”¨å…³é”®è¯
        if not all_datasets:
            logger.warning("âš ï¸ æ‰€æœ‰ç­–ç•¥å‡å¤±è´¥ï¼Œå°è¯•é€šç”¨å…³é”®è¯")
            generic_keywords = ["data", "dataset", "csv", "finance", "stock", "market", "crypto"]
            for gen_kw in generic_keywords:
                try:
                    datasets = self._searcher.search_by_keyword(gen_kw, max_results=3)
                    if datasets:
                        all_datasets.extend(datasets)
                        logger.info(f"âœ… é€šç”¨å…³é”®è¯'{gen_kw}'æ‰¾åˆ°{len(datasets)}ä¸ªç»“æœ")
                        break
                except Exception as e:
                    continue
        
        if not all_datasets:
            raise RuntimeError(f"âŒ æ‰€æœ‰å…³é”®è¯ç­–ç•¥å‡æ— æ£€ç´¢ç»“æœã€‚å°è¯•çš„å…³é”®è¯ï¼š{tried_keywords}")
        
        # 5. å»é‡æ•°æ®é›†ï¼ˆåŸºäºrefï¼‰
        seen_refs = set()
        unique_datasets = []
        for ds in all_datasets:
            if ds.ref not in seen_refs:
                seen_refs.add(ds.ref)
                unique_datasets.append(ds)
        
        logger.info(f"ğŸ“Š å…±æ‰¾åˆ°{len(unique_datasets)}ä¸ªå”¯ä¸€æ•°æ®é›†")
        
        # 6. é€‰æ‹©æœ€ä½³åŒ¹é…ï¼ˆä¼˜å…ˆé€‰æ‹©ä¸å…³é”®è¯æœ€ç›¸å…³çš„ï¼‰
        best_dataset = self._searcher.select_best_match(unique_datasets, keyword)
        if not best_dataset:
            # å¦‚æœé€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
            best_dataset = unique_datasets[0]
        
        logger.info(f"ğŸ“¥ é€‰æ‹©æ•°æ®é›†ï¼š{best_dataset.title} ({best_dataset.ref})")
        
        # 7. ä¸‹è½½è¯¥æ•°æ®é›†
        success, msg, output_path = self._downloader.download_dataset(
            best_dataset.ref, download_path, self._config.auto_extract
        )
        
        if not success:
            # å¦‚æœç¬¬ä¸€ä¸ªå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ•°æ®é›†
            logger.warning(f"âš ï¸ æ•°æ®é›†{best_dataset.ref}ä¸‹è½½å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ•°æ®é›†")
            for dataset in unique_datasets[:5]:  # æœ€å¤šå°è¯•5ä¸ª
                if dataset.ref == best_dataset.ref:
                    continue
                logger.info(f"ğŸ”„ å°è¯•ä¸‹è½½ï¼š{dataset.ref}")
                success, msg, output_path = self._downloader.download_dataset(
                    dataset.ref, download_path, self._config.auto_extract
                )
                if success and self._check_csv_exists(output_path):
                    best_dataset = dataset
                    break
        
        # 8. æ£€æµ‹CSVï¼ŒæˆåŠŸåˆ™è¿”å›
        if success and self._check_csv_exists(output_path):
            csv_files = list(Path(output_path).rglob("*.csv"))
            csv_file = csv_files[0] if csv_files else "æœªæ‰¾åˆ°"
            final_msg = f"""âœ… ä¸‹è½½å®Œæˆï¼
- åŸå§‹å…³é”®è¯ï¼š{keyword}
- ä½¿ç”¨å…³é”®è¯ï¼š{tried_keywords}
- æ•°æ®é›†ï¼š{best_dataset.title}
- æ•°æ®é›†å¼•ç”¨ï¼š{best_dataset.ref}
- è·¯å¾„ï¼š{output_path}
- CSVæ–‡ä»¶ï¼š{csv_file}"""
            return final_msg.strip(), output_path, best_dataset.url
        
        # 9. å¤±è´¥åˆ™æŠ¥é”™
        raise RuntimeError(f"""âŒ ä¸‹è½½å¤±è´¥ï¼
- å°è¯•çš„å…³é”®è¯ï¼š{tried_keywords}
- æ•°æ®é›†ï¼š{best_dataset.ref}
- åŸå› ï¼š{msg}ï¼ˆæœªæ‰¾åˆ°æœ‰æ•ˆCSVï¼‰""")

    async def _arun(self, **kwargs):
        import asyncio
        return await asyncio.get_event_loop().run_in_executor(None, lambda: self._run(** kwargs))

def create_kaggle_tool():
    def create_tool(state: State) -> State:
        # è·å–å…³é”®è¯ï¼Œæ”¯æŒå¤šç§æ¥æº
        raw_keyword = state.get("dataset", "").strip()
        
        # å¦‚æœdatasetå­—æ®µä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
        if not raw_keyword:
            # å°è¯•ä»topicã€new_ideaç­‰å­—æ®µè·å–
            raw_keyword = state.get("topic", "").strip()
            if not raw_keyword:
                raw_keyword = state.get("new_idea", "").strip()
            if not raw_keyword:
                # é»˜è®¤ä½¿ç”¨é€šç”¨å…³é”®è¯
                raw_keyword = "finance data"
        
        final_keyword = raw_keyword.strip()
        logger.info(f'ğŸ“Œ æœ€ç»ˆæ£€ç´¢å…³é”®è¯ï¼š{final_keyword}')

        # åˆå§‹åŒ–å·¥å…·
        config = KaggleToolConfig()
        dataset_tool = KaggleDatasetTool(config=config)
        
        # æ‰§è¡Œä¸‹è½½ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„å¤šçº§é™çº§ç­–ç•¥ï¼‰
        try:
            result, output_path, dataset_url = dataset_tool._run(
                keyword=final_keyword,
                download_path=state.get("download_path", "./datasets")
            )
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é›†æ£€ç´¢å¤±è´¥ï¼š{e}")
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨é€šç”¨å…³é”®è¯é‡æ–°æœç´¢...")
            try:
                result, output_path, dataset_url = dataset_tool._run(
                    keyword="data csv",
                    download_path=state.get("download_path", "./datasets")
                )
                logger.info("âœ… ä½¿ç”¨é€šç”¨å…³é”®è¯æˆåŠŸæ‰¾åˆ°æ•°æ®é›†")
            except Exception as e2:
                raise RuntimeError(f"âŒ æ•°æ®é›†æ£€ç´¢å®Œå…¨å¤±è´¥ï¼š{e2}")

        # å¤„ç†ç»“æœ
        state["input_data_path"] = output_path
        state["dataset_url"] = dataset_url
        state["download_status"] = result

        # æ ¡éªŒ
        if not dataset_tool._check_csv_exists(output_path):
            raise RuntimeError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆCSVæ–‡ä»¶ï¼")
        
        logger.info(f"âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸï¼š{output_path}")
        return state

    return create_tool

# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    # æ¨¡æ‹ŸStateç±»
    class State(dict):
        def get(self, key, default=None):
            return super().get(key, default)

    # ä¿®å¤æƒé™
    try:
        kaggle_json = Path.home() / ".kaggle" / "kaggle.json"
        if kaggle_json.exists():
            os.chmod(kaggle_json, 0o600)
            print("âœ… å·²ä¿®å¤kaggle.jsonæƒé™")
        else:
            raise FileNotFoundError("kaggle.jsonä¸å­˜åœ¨ï¼")
    except Exception as e:
        print(f"æƒé™é”™è¯¯ï¼š{e}")
        exit(1)

    # æµ‹è¯•æ‰§è¡Œï¼ˆä»…ä¸‹è½½1ä¸ªï¼‰
    tool = create_kaggle_tool()
    state = State({
        "dataset": "air pollution in beijing",
        "download_path": "/Users/chongyanghe/Desktop/DeepScientist/outputs/dataset"
    })

    try:
        result_state = tool(state)
        print("\nğŸ‰ ä»…ä¸‹è½½1ä¸ªæ•°æ®é›†æˆåŠŸï¼")
        print(f"è·¯å¾„ï¼š{result_state['input_data_path']}")
        print(f"çŠ¶æ€ï¼š{result_state['download_status']}")
    except Exception as e:
        print(f"\nâŒ å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()

from typing import List, Dict, Any
from pathlib import Path
from tools.document_segment import SegmentTool
from docling_core.types.doc import ImageRefMode
from utils.state import State
from common.utils import init_logger, get_pdf_files, ensure_dirs
import os
from loguru import logger
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

from langchain_core.load import dumps, loads

# logger = init_logger("pdf_parser_agent")

class PDFParser:
    def __init__(self, pdf_path, md_out_path, enable_formula_enrichment: bool = False):
        """
        åˆå§‹åŒ–PDFè§£æä»£ç†
        
        Args:
            enable_formula_enrichment: æ˜¯å¦å¯ç”¨å…¬å¼è¯†åˆ«ï¼ˆé»˜è®¤Falseï¼Œå› ä¸ºå¾ˆæ…¢ï¼‰
        """
        ensure_dirs()
        self.pdf_dir = pdf_path
        self.md_output_dir = md_out_path
        self.segment_tool = SegmentTool(enable_formula_enrichment=enable_formula_enrichment)
    
    def _is_pdf_cached(self, pdf_path: str, md_output_dir: str) -> bool:
        """
        æ£€æŸ¥PDFæ˜¯å¦å·²ç»è§£æè¿‡ï¼ˆç¼“å­˜æ£€æŸ¥ï¼‰
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            md_output_dir: Markdownè¾“å‡ºç›®å½•
        
        Returns:
            Trueå¦‚æœå·²ç¼“å­˜ä¸”PDFæœªä¿®æ”¹ï¼ŒFalseå¦åˆ™
        """
        pdf_file = Path(pdf_path)
        if not pdf_file.exists():
            return False
        
        file_stem = pdf_file.stem
        output_dir = Path(md_output_dir) / file_stem
        md_file = output_dir / f"{file_stem}.md"
        
        # å¦‚æœmarkdownæ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦è§£æ
        if not md_file.exists():
            return False
        
        # æ£€æŸ¥PDFæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
        pdf_mtime = pdf_file.stat().st_mtime
        md_mtime = md_file.stat().st_mtime
        
        # å¦‚æœPDFæ¯”markdownæ–°ï¼Œéœ€è¦é‡æ–°è§£æ
        if pdf_mtime > md_mtime:
            logger.info(f"ğŸ“„ PDFå·²æ›´æ–°ï¼Œéœ€è¦é‡æ–°è§£æ: {pdf_file.name}")
            return False
        
        return True
    
    def _convert_single_pdf(self, pdf_path: str, md_output_dir: str, use_cache: bool = True) -> Dict[str, Any]:
        """
        è½¬æ¢å•ä¸ªPDFæ–‡ä»¶ä¸ºMarkdown
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            md_output_dir: Markdownè¾“å‡ºç›®å½•
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆå¦‚æœå·²è§£æè¿‡åˆ™è·³è¿‡ï¼‰
        
        Returns:
            DictåŒ…å«è½¬æ¢ç»“æœä¿¡æ¯
        """
        pdf_file = Path(pdf_path)
        if not pdf_file.exists():
            raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        
        file_stem = pdf_file.stem
        output_dir = Path(md_output_dir) / file_stem
        output_dir.mkdir(parents=True, exist_ok=True)
        figs_dir = output_dir / "figs"
        figs_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and self._is_pdf_cached(pdf_path, md_output_dir):
            logger.info(f"âš¡ ä½¿ç”¨ç¼“å­˜: {pdf_file.name} (è·³è¿‡è§£æ)")
            md_file = output_dir / f"{file_stem}.md"
            content = ""
            if md_file.exists():
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            figures = []
            if figs_dir.exists():
                for f in sorted(figs_dir.glob("*.png")):
                    figures.append(str(f.relative_to(Path(md_output_dir))))
            
            page_images_relative = []
            pages_dir = output_dir / "pages"
            if pages_dir.exists():
                for f in sorted(pages_dir.glob("*.png")):
                    page_images_relative.append(str(f.relative_to(Path(md_output_dir))))
            
            md_path_relative = None
            if md_file.exists():
                md_path_relative = str(md_file.relative_to(Path(md_output_dir)))
            
            return {
                "pdf_path": pdf_path,
                "markdown_path": md_path_relative,
                "content": content,
                "figures": figures,
                "page_images": page_images_relative,
                "status": "success",
                "cached": True
            }
        
        logger.info(f"ğŸ”„ æ­£åœ¨å¤„ç†: {pdf_file.name} -> {output_dir}")
        
        # è½¬æ¢PDF
        conv_res = self.segment_tool.converter.convert(str(pdf_file))
        self.segment_tool._export_figures(conv_res, figs_dir, file_stem)
        
        # å°†PDFæ¯ä¸€é¡µè½¬æ¢ä¸ºå›¾ç‰‡ï¼ˆå·²æ³¨é‡Šï¼Œå› ä¸ºå¾ˆæ…¢ï¼‰
        page_images = []
        
        # å¯¼å‡ºMarkdown
        md_file = output_dir / f"{file_stem}.md"
        conv_res.document.save_as_markdown(
            str(md_file),
            image_mode=ImageRefMode.REFERENCED,
            artifacts_dir=figs_dir
        )
        
        content = ""
        if md_file.exists():
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
        
        figures = []
        if figs_dir.exists():
            for f in sorted(figs_dir.glob("*.png")):
                figures.append(str(f.relative_to(Path(md_output_dir))))
        
        page_images_relative = []
        pages_dir = output_dir / "pages"
        if pages_dir.exists():
            for f in sorted(pages_dir.glob("*.png")):
                page_images_relative.append(str(f.relative_to(Path(md_output_dir))))
        
        md_path_relative = None
        if md_file.exists():
            md_path_relative = str(md_file.relative_to(Path(md_output_dir)))
        
        return {
            "pdf_path": pdf_path,
            "markdown_path": md_path_relative,
            "content": content,
            "figures": figures,
            "page_images": page_images_relative,
            "status": "success",
            "cached": False
        }
    
    def run(self, state: State, max_workers: int = 4, use_cache: bool = True) -> State:
        """
        æ‰§è¡ŒPDFè§£ææµç¨‹ï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†å’Œç¼“å­˜ï¼‰
        
        Args:
            state: çŠ¶æ€å­—å…¸
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
        
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        """
        logger.info("ğŸš€ å¼€å§‹PDFè§£ææµç¨‹ï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰")
        
        pdf_files = state.get("downloaded_papers", [])
        if not pdf_files:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°éœ€è¦è§£æçš„PDFæ–‡ä»¶")
            return state
        
        logger.info(f"ğŸ“š å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        # å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œåˆ†ç¦»éœ€è¦è§£æå’Œå·²ç¼“å­˜çš„æ–‡ä»¶
        files_to_parse = []
        cached_results = []
        
        for pdf_path in pdf_files:
            if use_cache and self._is_pdf_cached(pdf_path, self.md_output_dir):
                # ç›´æ¥ä»ç¼“å­˜åŠ è½½
                try:
                    result = self._convert_single_pdf(pdf_path, self.md_output_dir, use_cache=True)
                    cached_results.append(result)
                    logger.info(f"âš¡ ä»ç¼“å­˜åŠ è½½: {Path(pdf_path).name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°è§£æ: {Path(pdf_path).name}, é”™è¯¯: {e}")
                    files_to_parse.append(pdf_path)
            else:
                files_to_parse.append(pdf_path)
        
        parsed_results = cached_results.copy()
        
        # å¹¶è¡Œå¤„ç†éœ€è¦è§£æçš„æ–‡ä»¶
        if files_to_parse:
            logger.info(f"ğŸ”„ éœ€è¦è§£æ {len(files_to_parse)} ä¸ªæ–‡ä»¶ï¼ˆå¹¶è¡Œå¤„ç†ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {max_workers}ï¼‰")
            
            def parse_single(pdf_path):
                """å•ä¸ªPDFè§£æä»»åŠ¡"""
                try:
                    result = self._convert_single_pdf(pdf_path, self.md_output_dir, use_cache=False)
                    logger.info(f"âœ“ æˆåŠŸè§£æ: {Path(pdf_path).name}")
                    return result
                except Exception as e:
                    error_msg = f"è§£æ{pdf_path}å¤±è´¥: {str(e)}"
                    logger.error(f"âŒ {error_msg}", exc_info=True)
                    return {
                        "pdf_path": pdf_path,
                        "markdown_path": None,
                        "content": "",
                        "figures": [],
                        "page_images": [],
                        "status": "failed",
                        "error": str(e)
                    }
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_pdf = {
                    executor.submit(parse_single, pdf_path): pdf_path 
                    for pdf_path in files_to_parse
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_pdf):
                    pdf_path = future_to_pdf[future]
                    try:
                        result = future.result()
                        parsed_results.append(result)
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç† {Path(pdf_path).name} æ—¶å‡ºé”™: {e}")
                parsed_results.append({
                    "pdf_path": pdf_path,
                    "markdown_path": None,
                    "content": "",
                    "figures": [],
                    "page_images": [],
                    "status": "failed",
                    "error": str(e)
                })
        
        # æ›´æ–°çŠ¶æ€
        if "parsed_multimodal_content" in state and state["parsed_multimodal_content"]:
            state["parsed_multimodal_content"].extend(parsed_results)
        else:
            state["parsed_multimodal_content"] = parsed_results
        
        # æ›´æ–°é”™è¯¯åˆ—è¡¨
        failed_results = [r for r in parsed_results if r.get("status") == "failed"]
        if failed_results:
            if "errors" not in state:
                state["errors"] = []
            for result in failed_results:
                if "error" in result:
                    state["errors"].append(result["error"])
        
        success_count = sum(1 for r in parsed_results if r.get("status") == "success")
        cached_count = sum(1 for r in parsed_results if r.get("cached", False))
        logger.info(f"âœ… PDFè§£æå®Œæˆ: æˆåŠŸ {success_count}/{len(parsed_results)} ä¸ªæ–‡ä»¶ (å…¶ä¸­ {cached_count} ä¸ªä½¿ç”¨ç¼“å­˜)")
        
        return state
